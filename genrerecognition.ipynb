{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the imports are not useful yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import tarfile\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from IPython.display import display, Image\n",
    "from scipy import ndimage\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "import tensorflow as tf\n",
    "from yaafelib import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download GTZAN dataset once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def downloadGTZAN():\n",
    "    filename = 'genres.tar.gz'\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urlretrieve('http://opihi.cs.uvic.ca/sound/genres.tar.gz', filename)\n",
    "    else:\n",
    "        print('File ' + filename + ' exists')\n",
    "    \n",
    "    return filename\n",
    "filename = downloadGTZAN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract files from .tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting genres.tar.gz \n",
      "['genres/blues', 'genres/classical', 'genres/country', 'genres/disco', 'genres/hiphop', 'genres/jazz', 'genres/metal', 'genres/pop', 'genres/reggae', 'genres/rock']\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "\n",
    "def extract(filename):\n",
    "    root = os.path.splitext(os.path.splitext(filename)[0])[0]  # remove .tar.gz\n",
    "    if os.path.isdir(root):\n",
    "        print('File %s already extracted' % (root))\n",
    "    else:\n",
    "        print('Extracting %s ' % filename)\n",
    "        tar = tarfile.open(filename)\n",
    "        sys.stdout.flush()\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "    data_folders = [os.path.join(root, d) for d in sorted(os.listdir(root))\n",
    "                    if os.path.isdir(os.path.join(root, d))]\n",
    "    if len(data_folders) != num_classes:\n",
    "        raise Exception('Expected %d folders not found.' % (num_classes))\n",
    "    print(data_folders)\n",
    "    return data_folders\n",
    "\n",
    "folders = extract(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_features(audio_file):\n",
    "    if os.path.exists(audio_file):\n",
    "        print('Getting features from ' + audio_file)\n",
    "    else:\n",
    "        raise Exception('File ' + audio_file + ' not found')\n",
    "    fp = FeaturePlan(sample_rate=22050, normalize=1)\n",
    "    # Features that seems to be most often used, so they are good to start with.\n",
    "    fp.addFeature('mfcc: MFCC')\n",
    "    fp.addFeature('zcr: ZCR')\n",
    "    fp.addFeature('spectral_shape: SpectralShapeStatistics')\n",
    "    fp.addFeature('magnitude_spectrum: MagnitudeSpectrum')\n",
    "    df = fp.getDataFlow()\n",
    "    engine = Engine()\n",
    "    engine.load(df)\n",
    "    afp = AudioFileProcessor()\n",
    "    afp.setOutputFormat('csv', 'features', {'Precision': '8', 'Metadata': 'False'})\n",
    "    afp.processFile(engine, audio_file)\n",
    "    engine.flush()\n",
    "    feats = engine.readAllOutputs()\n",
    "    return feats\n",
    "\n",
    "def load_genre(folder):\n",
    "    print('Loading genre from folder ' + folder)\n",
    "    samples = os.listdir(folder)\n",
    "    dataset =[]\n",
    "    for sample in os.listdir(folder):\n",
    "        sample_file = os.path.join(folder, sample)\n",
    "        if sample.endswith('.au'):\n",
    "            features = get_features(sample_file)\n",
    "            dataset.append(features)\n",
    "    return dataset\n",
    "\n",
    "def pickle(data_folders):\n",
    "    dataset_names = []\n",
    "    for folder in data_folders:\n",
    "        set_filename = folder + '.pickle'\n",
    "        dataset_names.append(set_filename)\n",
    "        if os.path.exists(set_filename):\n",
    "            print('%s already pickled' % set_filename)\n",
    "        else:\n",
    "            print('Pickling %s.' % set_filename)\n",
    "            dataset = load_genre(folder)\n",
    "            with open(set_filename, 'wb') as f:\n",
    "                pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "    return dataset_names\n",
    "\n",
    "pickled_datasets = pickle(folders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the data is pickled, thus the feature extraction doesn't have to be repeated. \n",
    "\n",
    "Let's vizualize something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "\n",
    "def visualize(filename, sample, feature):\n",
    "    with open(filename, 'r') as f:\n",
    "        unpickled = pickle.load(f)\n",
    "        f = unpickled[sample][feature]\n",
    "        plt.plot(f[0])\n",
    "        plt.ylabel(feature)\n",
    "        plt.show()\n",
    "\n",
    "visualize('genres/rock.pickle', 0, 'magnitude_spectrum')\n",
    "visualize('genres/rock.pickle', 1, 'magnitude_spectrum')\n",
    "visualize('genres/rock.pickle', 2, 'magnitude_spectrum')\n",
    "visualize('genres/rock.pickle', 3, 'magnitude_spectrum')\n",
    "visualize('genres/blues.pickle', 0, 'magnitude_spectrum')\n",
    "visualize('genres/blues.pickle', 1, 'magnitude_spectrum')\n",
    "visualize('genres/blues.pickle', 2, 'magnitude_spectrum')\n",
    "visualize('genres/blues.pickle', 3, 'magnitude_spectrum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training set and test set are needed. In addition, they have to be shuffeled/randomized somehow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickled_datasets = ['blues.pickle','classical.pickle','country.pickle','disco.pickle','hiphop.pickle','jazz.pickle',\n",
    "                    'metal.pickle','pop.pickle','reggae.pickle','rock.pickle']\n",
    "\n",
    "def unpickle(filename):\n",
    "    with open('genres/' + filename, 'r') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "def evens(dataset):\n",
    "    return dataset[::2]\n",
    "def odds(dataset):\n",
    "    return dataset[1::2]\n",
    "\n",
    "def dataset_with_labels(filename):\n",
    "    dataset = unpickle(filename)\n",
    "    return dataset, [os.path.splitext(filename)[0] for count in xrange(len(dataset))]\n",
    "\n",
    "datasets = map(dataset_with_labels, pickled_datasets)\n",
    "features = [features for genre in datasets for features in genre[0]]\n",
    "labels = [label for genre in datasets for label in genre[1]]\n",
    "\n",
    "train_dataset = odds(features)\n",
    "train_labels = odds(labels)\n",
    "test_dataset = evens(features)\n",
    "test_labels = evens(labels)\n",
    "\n",
    "def randomize(dataset, labels):\n",
    "    permutation = np.random.permutation(len(labels))\n",
    "    shuffled_dataset = np.asarray(dataset)[permutation]\n",
    "    shuffled_labels = np.asarray(labels)[permutation]\n",
    "    return shuffled_dataset, shuffled_labels\n",
    "\n",
    "train_dataset, train_labels = randomize(train_dataset, train_labels)\n",
    "test_dataset, test_labels = randomize(test_dataset, test_labels)\n",
    "\n",
    "try:\n",
    "    f = open('train_and_test_data.pickle', 'wb')\n",
    "    save = {\n",
    "        'train_features': train_dataset,\n",
    "        'train_labels': train_labels,\n",
    "        'test_features': test_dataset,\n",
    "        'test_labels': test_labels,\n",
    "    }\n",
    "    pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n",
    "except Exception as e:\n",
    "    print('Unable to save data to', pickle_file, ':', e)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can go to TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can start with\n",
    "====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('train_and_test_data.pickle', 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_features']\n",
    "    train_labels = save['train_labels']\n",
    "    test_dataset = save['test_features']\n",
    "    test_labels = save['test_labels']\n",
    "    del save  # gc\n",
    "train_dataset = ([d['magnitude_spectrum'][0] for d in train_dataset])\n",
    "test_dataset = ([d['magnitude_spectrum'][0] for d in test_dataset])\n",
    "magnitude = 513\n",
    "num_labels = 10 # genres\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def oneHotEncoder(pos, max):\n",
    "    encoded = []\n",
    "    for i in range(0, max):\n",
    "        if i == pos:\n",
    "            encoded.append(1)\n",
    "        else:\n",
    "            encoded.append(0)\n",
    "    return encoded\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = np.asarray(dataset).reshape((-1, magnitude, 1, 1)).astype(np.float32)\n",
    "    genres = ['blues','classical','country','disco','hiphop','jazz','metal','pop','reggae','rock']\n",
    "    labels = map(lambda x: np.int32(genres.index(x)), labels)\n",
    "    labels = map(lambda x: oneHotEncoder(x, num_labels), labels)\n",
    "    return dataset, labels\n",
    "\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Params\n",
    "learning_rate = 0.001\n",
    "training_iters = 400 # accuracy started decreasing for larger numbers of iterations\n",
    "batch_size = 4\n",
    "display_step = 10\n",
    "\n",
    "# Network Params\n",
    "n_input = 513\n",
    "n_classes = 10 # genres \n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float32, [None, n_input, None, None])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "def conv2d(features, w, b):\n",
    "    return tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(features, w, strides=[1, 1, 1, 1], padding='SAME'),b))\n",
    "\n",
    "def max_pool(features, k):\n",
    "    return tf.nn.max_pool(features, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')\n",
    "\n",
    "def conv_net(_X, _weights, _biases):\n",
    "    _X = tf.reshape(_X, shape=[-1,513,1,1])\n",
    "\n",
    "    conv1 = conv2d(_X, _weights['wc1'], _biases['bc1'])\n",
    "    conv1 = max_pool(conv1, k=1)\n",
    "\n",
    "    conv2 = conv2d(conv1, _weights['wc2'], _biases['bc2'])\n",
    "    conv2 = max_pool(conv2, k=1)\n",
    "\n",
    "    dense1 = tf.reshape(conv2, [-1, _weights['wd1'].get_shape().as_list()[0]]) \n",
    "    dense1 = tf.nn.relu(tf.add(tf.matmul(dense1, _weights['wd1']), _biases['bd1']))\n",
    "    \n",
    "    out = tf.add(tf.matmul(dense1, _weights['out']), _biases['out'])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a huge field for improvements here:\n",
    "* Starting points should not be randomized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights = {\n",
    "    # 5x1 conv, 1 input, 32 outputs\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 1, 1, 32])), \n",
    "    # 5x1 conv, 32 inputs, 64 outputs\n",
    "    'wc2': tf.Variable(tf.random_normal([5, 1, 32, 64])), \n",
    "    # fully connected, 7*7*64 inputs, 1024 outputs\n",
    "    'wd1': tf.Variable(tf.random_normal([64 * 513, 1024])), \n",
    "    # 1024 inputs, 10 outputs (class prediction)\n",
    "    'out': tf.Variable(tf.random_normal([1024, n_classes])) \n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'bc2': tf.Variable(tf.random_normal([64])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "pred = conv_net(x, weights, biases)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Training & Testing:\n",
    "========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 40, Minibatch Loss= 12503.488281, Training Accuracy= 0.36000\n",
      "Iter 80, Minibatch Loss= 4351.641113, Training Accuracy= 0.54800\n",
      "Iter 120, Minibatch Loss= 1680.632446, Training Accuracy= 0.69200\n",
      "Iter 160, Minibatch Loss= 772.510498, Training Accuracy= 0.78000\n",
      "Iter 200, Minibatch Loss= 384.756317, Training Accuracy= 0.86600\n",
      "Iter 240, Minibatch Loss= 179.520569, Training Accuracy= 0.90200\n",
      "Iter 280, Minibatch Loss= 88.095207, Training Accuracy= 0.93200\n",
      "Iter 320, Minibatch Loss= 23.648977, Training Accuracy= 0.96800\n",
      "Iter 360, Minibatch Loss= 5.767161, Training Accuracy= 0.97400\n",
      "Testing Accuracy: 0.24\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "    while step * batch_size < training_iters:        \n",
    "        sess.run(optimizer, feed_dict={x: train_dataset, y: train_labels})\n",
    "        if step % display_step == 0:\n",
    "            acc = sess.run(accuracy, feed_dict={x: train_dataset, y: train_labels})\n",
    "            loss = sess.run(cost, feed_dict={x: train_dataset, y: train_labels})\n",
    "            print(\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \"{:.6f}\".format(loss) + \\\n",
    "            \", Training Accuracy= \" + \"{:.5f}\".format(acc))\n",
    "        step += 1\n",
    "    print(\"Testing Accuracy:\", sess.run(accuracy, feed_dict={x: test_dataset, y: test_labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Classifier is disastrous. Has to be enhanced, but it's somehow working..."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "2_fullyconnected.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
