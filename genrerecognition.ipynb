{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the imports are not useful yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import tarfile\n",
    "from IPython.display import display, Image\n",
    "from scipy import ndimage\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "import tensorflow as tf\n",
    "from yaafelib import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download GTZAN dataset once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def downloadGTZAN():\n",
    "    filename = 'genres.tar.gz'\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urlretrieve('http://opihi.cs.uvic.ca/sound/genres.tar.gz', filename)\n",
    "    else:\n",
    "        print('File ' + filename + ' exists')\n",
    "    \n",
    "    return filename\n",
    "filename = downloadGTZAN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract files from .tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting genres.tar.gz \n",
      "['genres/blues', 'genres/classical', 'genres/country', 'genres/disco', 'genres/hiphop', 'genres/jazz', 'genres/metal', 'genres/pop', 'genres/reggae', 'genres/rock']\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "\n",
    "def extract(filename):\n",
    "    root = os.path.splitext(os.path.splitext(filename)[0])[0]  # remove .tar.gz\n",
    "    if os.path.isdir(root):\n",
    "        print('File %s already extracted' % (root))\n",
    "    else:\n",
    "        print('Extracting %s ' % filename)\n",
    "        tar = tarfile.open(filename)\n",
    "        sys.stdout.flush()\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "    data_folders = [os.path.join(root, d) for d in sorted(os.listdir(root))\n",
    "                    if os.path.isdir(os.path.join(root, d))]\n",
    "    if len(data_folders) != num_classes:\n",
    "        raise Exception('Expected %d folders not found.' % (num_classes))\n",
    "    print(data_folders)\n",
    "    return data_folders\n",
    "\n",
    "folders = extract(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_features(audio_file):\n",
    "    if os.path.exists(audio_file):\n",
    "        print('Getting features from ' + audio_file)\n",
    "    else:\n",
    "        raise Exception('File ' + audio_file + ' not found')\n",
    "    fp = FeaturePlan(sample_rate=22050, normalize=1)\n",
    "    # Features that seems to be most often used, so they are good to start with.\n",
    "    fp.addFeature('mfcc: MFCC')\n",
    "    fp.addFeature('zcr: ZCR')\n",
    "    fp.addFeature('spectral_shape: SpectralShapeStatistics')\n",
    "    fp.addFeature('magnitude_spectrum: MagnitudeSpectrum')\n",
    "    df = fp.getDataFlow()\n",
    "    engine = Engine()\n",
    "    engine.load(df)\n",
    "    afp = AudioFileProcessor()\n",
    "    afp.setOutputFormat('csv', 'features', {'Precision': '8', 'Metadata': 'False'})\n",
    "    afp.processFile(engine, audio_file)\n",
    "    engine.flush()\n",
    "    feats = engine.readAllOutputs()\n",
    "    return feats\n",
    "\n",
    "def load_genre(folder):\n",
    "    print('Loading genre from folder ' + folder)\n",
    "    samples = os.listdir(folder)\n",
    "    dataset =[]\n",
    "    for sample in os.listdir(folder):\n",
    "        sample_file = os.path.join(folder, sample)\n",
    "        if sample.endswith('.au'):\n",
    "            features = get_features(sample_file)\n",
    "            dataset.append(features)\n",
    "    return dataset\n",
    "\n",
    "def pickle(data_folders):\n",
    "    dataset_names = []\n",
    "    for folder in data_folders:\n",
    "        set_filename = folder + '.pickle'\n",
    "        dataset_names.append(set_filename)\n",
    "        if os.path.exists(set_filename):\n",
    "            print('%s already pickled' % set_filename)\n",
    "        else:\n",
    "            print('Pickling %s.' % set_filename)\n",
    "            dataset = load_genre(folder)\n",
    "            with open(set_filename, 'wb') as f:\n",
    "                pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "    return dataset_names\n",
    "\n",
    "pickled_datasets = pickle(folders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the data is pickled, thus the feature extraction doesn't have to be repeated. \n",
    "\n",
    "Let's vizualize something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "\n",
    "def visualize(filename, sample, feature):\n",
    "    with open(filename, 'r') as f:\n",
    "        unpickled = pickle.load(f)\n",
    "        f = unpickled[sample][feature]\n",
    "        plt.plot(f[0])\n",
    "        plt.ylabel(feature)\n",
    "        plt.show()\n",
    "\n",
    "visualize('genres/rock.pickle', 0, 'magnitude_spectrum')\n",
    "visualize('genres/rock.pickle', 1, 'magnitude_spectrum')\n",
    "visualize('genres/rock.pickle', 2, 'magnitude_spectrum')\n",
    "visualize('genres/rock.pickle', 3, 'magnitude_spectrum')\n",
    "visualize('genres/blues.pickle', 0, 'magnitude_spectrum')\n",
    "visualize('genres/blues.pickle', 1, 'magnitude_spectrum')\n",
    "visualize('genres/blues.pickle', 2, 'magnitude_spectrum')\n",
    "visualize('genres/blues.pickle', 3, 'magnitude_spectrum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training set and test set are needed. In addition, they have to be shuffeled/randomized somehow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickled_datasets = ['blues.pickle','classical.pickle','country.pickle','disco.pickle','hiphop.pickle','jazz.pickle',\n",
    "                    'metal.pickle','pop.pickle','reggae.pickle','rock.pickle']\n",
    "\n",
    "def unpickle(filename):\n",
    "    with open('genres/' + filename, 'r') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "def evens(dataset):\n",
    "    return dataset[::2]\n",
    "def odds(dataset):\n",
    "    return dataset[1::2]\n",
    "\n",
    "def dataset_with_labels(filename):\n",
    "    dataset = unpickle(filename)\n",
    "    return dataset, [os.path.splitext(filename)[0] for count in xrange(len(dataset))]\n",
    "\n",
    "datasets = map(dataset_with_labels, pickled_datasets)\n",
    "features = [features for genre in datasets for features in genre[0]]\n",
    "labels = [label for genre in datasets for label in genre[1]]\n",
    "\n",
    "train_dataset = odds(features)\n",
    "train_labels = odds(labels)\n",
    "test_dataset = evens(features)\n",
    "test_labels = evens(labels)\n",
    "\n",
    "def randomize(dataset, labels):\n",
    "    permutation = np.random.permutation(len(labels))\n",
    "    shuffled_dataset = np.asarray(dataset)[permutation]\n",
    "    shuffled_labels = np.asarray(labels)[permutation]\n",
    "    return shuffled_dataset, shuffled_labels\n",
    "\n",
    "train_dataset, train_labels = randomize(train_dataset, train_labels)\n",
    "test_dataset, test_labels = randomize(test_dataset, test_labels)\n",
    "\n",
    "try:\n",
    "    f = open('train_and_test_data.pickle', 'wb')\n",
    "    save = {\n",
    "        'train_features': train_dataset,\n",
    "        'train_labels': train_labels,\n",
    "        'test_features': test_dataset,\n",
    "        'test_labels': test_labels,\n",
    "    }\n",
    "    pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n",
    "except Exception as e:\n",
    "    print('Unable to save data to', pickle_file, ':', e)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can go to TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('train_and_test_data.pickle', 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_features']\n",
    "    train_labels = save['train_labels']\n",
    "    test_dataset = save['test_features']\n",
    "    test_labels = save['test_labels']\n",
    "    del save  # gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dataset = ([d['magnitude_spectrum'][0] for d in train_dataset])\n",
    "test_dataset = ([d['magnitude_spectrum'][0] for d in test_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "(500, 513, 1, 1)\n",
      "500\n",
      "(500, 513, 1, 1)\n",
      "Training set [[[[  1.52177095e-01]]\n",
      "\n",
      "  [[  2.07503527e-01]]\n",
      "\n",
      "  [[  3.19659591e-01]]\n",
      "\n",
      "  ..., \n",
      "  [[  4.35094424e-02]]\n",
      "\n",
      "  [[  4.35185470e-02]]\n",
      "\n",
      "  [[  4.35238220e-02]]]\n",
      "\n",
      "\n",
      " [[[  5.45761347e-01]]\n",
      "\n",
      "  [[  6.29493654e-01]]\n",
      "\n",
      "  [[  8.05441797e-01]]\n",
      "\n",
      "  ..., \n",
      "  [[  4.54530149e-04]]\n",
      "\n",
      "  [[  3.63788451e-04]]\n",
      "\n",
      "  [[  3.37551872e-04]]]\n",
      "\n",
      "\n",
      " [[[  4.53602672e-02]]\n",
      "\n",
      "  [[  1.05932377e-01]]\n",
      "\n",
      "  [[  1.88352823e-01]]\n",
      "\n",
      "  ..., \n",
      "  [[  4.75088321e-02]]\n",
      "\n",
      "  [[  4.76097949e-02]]\n",
      "\n",
      "  [[  4.76567522e-02]]]\n",
      "\n",
      "\n",
      " ..., \n",
      " [[[  1.00901604e+00]]\n",
      "\n",
      "  [[  1.03644562e+00]]\n",
      "\n",
      "  [[  1.09161758e+00]]\n",
      "\n",
      "  ..., \n",
      "  [[  1.19346306e-02]]\n",
      "\n",
      "  [[  1.19335419e-02]]\n",
      "\n",
      "  [[  1.19327502e-02]]]\n",
      "\n",
      "\n",
      " [[[  1.72035873e+00]]\n",
      "\n",
      "  [[  1.99917138e+00]]\n",
      "\n",
      "  [[  2.70421362e+00]]\n",
      "\n",
      "  ..., \n",
      "  [[  2.32218988e-02]]\n",
      "\n",
      "  [[  2.32224502e-02]]\n",
      "\n",
      "  [[  2.32254341e-02]]]\n",
      "\n",
      "\n",
      " [[[  8.11217880e+00]]\n",
      "\n",
      "  [[  9.95153904e+00]]\n",
      "\n",
      "  [[  1.43381548e+01]]\n",
      "\n",
      "  ..., \n",
      "  [[  1.58054933e-01]]\n",
      "\n",
      "  [[  1.57763019e-01]]\n",
      "\n",
      "  [[  1.57447070e-01]]]] [9.0, 1.0, 5.0, 9.0, 4.0, 1.0, 1.0, 7.0, 8.0, 6.0, 5.0, 5.0, 7.0, 1.0, 2.0, 6.0, 0.0, 5.0, 3.0, 9.0, 8.0, 7.0, 7.0, 4.0, 3.0, 9.0, 7.0, 3.0, 4.0, 5.0, 5.0, 1.0, 9.0, 6.0, 9.0, 0.0, 8.0, 7.0, 0.0, 6.0, 8.0, 8.0, 4.0, 5.0, 9.0, 3.0, 4.0, 4.0, 3.0, 8.0, 6.0, 7.0, 9.0, 2.0, 1.0, 8.0, 2.0, 0.0, 5.0, 4.0, 8.0, 7.0, 1.0, 4.0, 3.0, 4.0, 8.0, 4.0, 3.0, 6.0, 3.0, 5.0, 6.0, 1.0, 1.0, 5.0, 6.0, 5.0, 7.0, 8.0, 3.0, 3.0, 2.0, 3.0, 7.0, 9.0, 4.0, 9.0, 6.0, 8.0, 1.0, 8.0, 7.0, 5.0, 4.0, 8.0, 5.0, 8.0, 0.0, 2.0, 3.0, 6.0, 6.0, 4.0, 8.0, 6.0, 2.0, 1.0, 7.0, 2.0, 2.0, 0.0, 0.0, 2.0, 8.0, 5.0, 2.0, 4.0, 9.0, 0.0, 9.0, 4.0, 4.0, 0.0, 6.0, 1.0, 6.0, 0.0, 2.0, 0.0, 9.0, 1.0, 0.0, 3.0, 1.0, 4.0, 9.0, 5.0, 3.0, 1.0, 3.0, 6.0, 7.0, 9.0, 5.0, 8.0, 4.0, 2.0, 7.0, 2.0, 9.0, 4.0, 3.0, 8.0, 4.0, 9.0, 5.0, 7.0, 2.0, 6.0, 6.0, 7.0, 3.0, 5.0, 7.0, 0.0, 1.0, 8.0, 2.0, 7.0, 2.0, 2.0, 5.0, 4.0, 4.0, 9.0, 1.0, 4.0, 3.0, 8.0, 8.0, 0.0, 0.0, 0.0, 6.0, 1.0, 9.0, 1.0, 6.0, 4.0, 4.0, 0.0, 3.0, 4.0, 2.0, 6.0, 3.0, 2.0, 3.0, 8.0, 7.0, 0.0, 2.0, 0.0, 2.0, 9.0, 0.0, 1.0, 2.0, 5.0, 5.0, 3.0, 0.0, 1.0, 5.0, 2.0, 3.0, 5.0, 7.0, 3.0, 1.0, 2.0, 3.0, 5.0, 9.0, 7.0, 9.0, 4.0, 7.0, 7.0, 2.0, 1.0, 6.0, 7.0, 8.0, 4.0, 9.0, 6.0, 3.0, 2.0, 1.0, 1.0, 6.0, 6.0, 4.0, 8.0, 8.0, 8.0, 4.0, 9.0, 5.0, 3.0, 4.0, 3.0, 2.0, 2.0, 3.0, 3.0, 3.0, 6.0, 3.0, 2.0, 5.0, 5.0, 6.0, 9.0, 0.0, 5.0, 9.0, 0.0, 8.0, 6.0, 7.0, 6.0, 3.0, 8.0, 9.0, 1.0, 1.0, 1.0, 7.0, 9.0, 5.0, 1.0, 5.0, 6.0, 1.0, 6.0, 7.0, 7.0, 9.0, 2.0, 7.0, 0.0, 7.0, 5.0, 1.0, 3.0, 0.0, 5.0, 2.0, 9.0, 1.0, 6.0, 9.0, 4.0, 9.0, 1.0, 1.0, 6.0, 2.0, 0.0, 0.0, 2.0, 2.0, 8.0, 0.0, 6.0, 6.0, 0.0, 6.0, 4.0, 8.0, 3.0, 0.0, 5.0, 8.0, 3.0, 0.0, 8.0, 0.0, 7.0, 5.0, 0.0, 0.0, 1.0, 5.0, 8.0, 6.0, 1.0, 7.0, 0.0, 6.0, 0.0, 9.0, 8.0, 8.0, 8.0, 5.0, 1.0, 5.0, 7.0, 4.0, 9.0, 9.0, 0.0, 2.0, 0.0, 3.0, 4.0, 7.0, 9.0, 8.0, 7.0, 0.0, 8.0, 9.0, 8.0, 2.0, 2.0, 0.0, 1.0, 4.0, 4.0, 4.0, 5.0, 7.0, 6.0, 1.0, 6.0, 4.0, 3.0, 2.0, 1.0, 4.0, 2.0, 4.0, 9.0, 9.0, 5.0, 4.0, 5.0, 2.0, 3.0, 0.0, 4.0, 9.0, 3.0, 5.0, 7.0, 7.0, 8.0, 8.0, 5.0, 6.0, 1.0, 7.0, 6.0, 1.0, 3.0, 6.0, 2.0, 2.0, 4.0, 7.0, 6.0, 2.0, 7.0, 1.0, 9.0, 3.0, 7.0, 9.0, 6.0, 5.0, 0.0, 4.0, 7.0, 9.0, 3.0, 1.0, 8.0, 5.0, 9.0, 7.0, 1.0, 2.0, 8.0, 3.0, 8.0, 5.0, 1.0, 5.0, 3.0, 8.0, 5.0, 2.0, 1.0, 1.0, 3.0, 2.0, 0.0, 7.0, 0.0, 9.0, 9.0, 7.0, 9.0, 9.0, 2.0, 3.0, 2.0, 7.0, 6.0, 3.0, 7.0, 8.0, 7.0, 7.0, 0.0, 5.0, 9.0, 5.0, 4.0, 6.0, 9.0, 4.0, 1.0, 6.0, 8.0, 3.0, 5.0, 0.0, 8.0, 0.0, 6.0, 0.0, 2.0, 0.0, 1.0, 4.0, 4.0, 3.0, 0.0, 8.0, 6.0, 4.0, 8.0, 2.0, 6.0]\n",
      "Test set [[[[  1.19721336e+01]]\n",
      "\n",
      "  [[  1.13198051e+01]]\n",
      "\n",
      "  [[  1.05098515e+01]]\n",
      "\n",
      "  ..., \n",
      "  [[  2.81342596e-01]]\n",
      "\n",
      "  [[  1.01992525e-01]]\n",
      "\n",
      "  [[  1.90554708e-01]]]\n",
      "\n",
      "\n",
      " [[[  1.79757512e+00]]\n",
      "\n",
      "  [[  1.80552197e+00]]\n",
      "\n",
      "  [[  1.92841864e+00]]\n",
      "\n",
      "  ..., \n",
      "  [[  6.17577322e-02]]\n",
      "\n",
      "  [[  8.36997628e-02]]\n",
      "\n",
      "  [[  9.41129327e-02]]]\n",
      "\n",
      "\n",
      " [[[  2.51174951e+00]]\n",
      "\n",
      "  [[  2.64529085e+00]]\n",
      "\n",
      "  [[  2.92268133e+00]]\n",
      "\n",
      "  ..., \n",
      "  [[  4.82440460e-03]]\n",
      "\n",
      "  [[  4.79941303e-03]]\n",
      "\n",
      "  [[  4.78744134e-03]]]\n",
      "\n",
      "\n",
      " ..., \n",
      " [[[  4.49370146e+00]]\n",
      "\n",
      "  [[  4.57556009e+00]]\n",
      "\n",
      "  [[  4.75738192e+00]]\n",
      "\n",
      "  ..., \n",
      "  [[  5.97208254e-02]]\n",
      "\n",
      "  [[  5.97143583e-02]]\n",
      "\n",
      "  [[  5.97103462e-02]]]\n",
      "\n",
      "\n",
      " [[[  3.87278485e+00]]\n",
      "\n",
      "  [[  4.49178314e+00]]\n",
      "\n",
      "  [[  5.85417080e+00]]\n",
      "\n",
      "  ..., \n",
      "  [[  1.71602387e-02]]\n",
      "\n",
      "  [[  1.71482526e-02]]\n",
      "\n",
      "  [[  1.71430670e-02]]]\n",
      "\n",
      "\n",
      " [[[  1.83061862e+00]]\n",
      "\n",
      "  [[  1.94634509e+00]]\n",
      "\n",
      "  [[  2.16131234e+00]]\n",
      "\n",
      "  ..., \n",
      "  [[  2.44311173e-03]]\n",
      "\n",
      "  [[  2.43743998e-03]]\n",
      "\n",
      "  [[  2.43678968e-03]]]] [8.0, 3.0, 9.0, 5.0, 8.0, 9.0, 7.0, 0.0, 9.0, 9.0, 9.0, 6.0, 2.0, 3.0, 7.0, 9.0, 3.0, 1.0, 3.0, 6.0, 9.0, 2.0, 3.0, 7.0, 0.0, 7.0, 6.0, 7.0, 8.0, 6.0, 7.0, 9.0, 1.0, 4.0, 7.0, 6.0, 9.0, 6.0, 2.0, 7.0, 5.0, 0.0, 5.0, 6.0, 2.0, 7.0, 1.0, 5.0, 5.0, 1.0, 4.0, 4.0, 2.0, 7.0, 7.0, 1.0, 8.0, 3.0, 7.0, 8.0, 7.0, 5.0, 8.0, 9.0, 7.0, 0.0, 4.0, 0.0, 2.0, 0.0, 4.0, 9.0, 0.0, 7.0, 5.0, 7.0, 7.0, 4.0, 9.0, 3.0, 7.0, 9.0, 2.0, 5.0, 2.0, 5.0, 4.0, 9.0, 1.0, 6.0, 8.0, 2.0, 2.0, 9.0, 7.0, 5.0, 5.0, 0.0, 6.0, 6.0, 1.0, 2.0, 2.0, 5.0, 2.0, 1.0, 4.0, 7.0, 0.0, 5.0, 7.0, 2.0, 3.0, 7.0, 0.0, 2.0, 9.0, 3.0, 1.0, 3.0, 8.0, 2.0, 4.0, 3.0, 8.0, 1.0, 4.0, 7.0, 8.0, 1.0, 6.0, 1.0, 6.0, 2.0, 5.0, 4.0, 7.0, 7.0, 0.0, 1.0, 5.0, 8.0, 7.0, 3.0, 6.0, 9.0, 0.0, 7.0, 3.0, 3.0, 6.0, 1.0, 9.0, 7.0, 6.0, 0.0, 9.0, 2.0, 2.0, 1.0, 8.0, 8.0, 6.0, 6.0, 4.0, 5.0, 7.0, 0.0, 7.0, 0.0, 1.0, 9.0, 5.0, 6.0, 4.0, 4.0, 7.0, 8.0, 6.0, 8.0, 6.0, 8.0, 2.0, 6.0, 4.0, 3.0, 3.0, 1.0, 8.0, 6.0, 4.0, 5.0, 3.0, 3.0, 3.0, 3.0, 6.0, 3.0, 3.0, 2.0, 6.0, 8.0, 0.0, 1.0, 9.0, 9.0, 5.0, 2.0, 3.0, 6.0, 7.0, 2.0, 2.0, 6.0, 7.0, 3.0, 3.0, 0.0, 4.0, 3.0, 6.0, 7.0, 4.0, 0.0, 6.0, 4.0, 2.0, 4.0, 4.0, 3.0, 6.0, 6.0, 9.0, 8.0, 4.0, 5.0, 7.0, 3.0, 9.0, 0.0, 0.0, 5.0, 9.0, 6.0, 4.0, 1.0, 4.0, 5.0, 3.0, 5.0, 8.0, 7.0, 9.0, 3.0, 3.0, 1.0, 5.0, 8.0, 8.0, 4.0, 4.0, 2.0, 7.0, 2.0, 4.0, 3.0, 7.0, 6.0, 6.0, 7.0, 9.0, 1.0, 1.0, 5.0, 8.0, 0.0, 4.0, 8.0, 7.0, 4.0, 5.0, 8.0, 5.0, 2.0, 1.0, 5.0, 8.0, 0.0, 2.0, 8.0, 2.0, 1.0, 1.0, 4.0, 2.0, 7.0, 0.0, 0.0, 4.0, 0.0, 1.0, 1.0, 4.0, 5.0, 2.0, 8.0, 8.0, 2.0, 0.0, 1.0, 1.0, 1.0, 3.0, 9.0, 5.0, 5.0, 1.0, 2.0, 7.0, 0.0, 1.0, 7.0, 8.0, 3.0, 1.0, 9.0, 5.0, 8.0, 2.0, 6.0, 6.0, 9.0, 8.0, 4.0, 4.0, 5.0, 9.0, 2.0, 9.0, 3.0, 0.0, 3.0, 8.0, 5.0, 6.0, 2.0, 2.0, 2.0, 5.0, 6.0, 0.0, 7.0, 4.0, 2.0, 3.0, 9.0, 9.0, 6.0, 0.0, 1.0, 0.0, 8.0, 4.0, 8.0, 6.0, 0.0, 3.0, 9.0, 5.0, 1.0, 1.0, 5.0, 8.0, 6.0, 1.0, 1.0, 5.0, 8.0, 2.0, 2.0, 5.0, 4.0, 0.0, 4.0, 4.0, 6.0, 8.0, 5.0, 7.0, 1.0, 9.0, 8.0, 4.0, 4.0, 9.0, 0.0, 5.0, 0.0, 1.0, 3.0, 1.0, 6.0, 0.0, 9.0, 1.0, 5.0, 7.0, 6.0, 8.0, 0.0, 9.0, 0.0, 8.0, 1.0, 3.0, 1.0, 2.0, 6.0, 1.0, 9.0, 2.0, 4.0, 4.0, 6.0, 0.0, 2.0, 3.0, 5.0, 3.0, 3.0, 4.0, 5.0, 9.0, 0.0, 7.0, 4.0, 1.0, 9.0, 2.0, 8.0, 0.0, 3.0, 5.0, 9.0, 6.0, 0.0, 0.0, 0.0, 7.0, 2.0, 9.0, 3.0, 5.0, 9.0, 4.0, 1.0, 7.0, 3.0, 5.0, 6.0, 8.0, 9.0, 5.0, 5.0, 5.0, 6.0, 4.0, 9.0, 8.0, 6.0, 9.0, 7.0, 0.0, 8.0, 0.0, 0.0, 2.0, 4.0, 4.0, 9.0, 6.0, 3.0, 8.0, 0.0, 1.0, 4.0, 2.0, 8.0, 0.0, 3.0, 9.0, 3.0, 1.0, 8.0, 1.0, 3.0, 2.0, 8.0, 8.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "magnitude = 513\n",
    "num_labels = 10 # genres\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = np.asarray(dataset).reshape((-1, magnitude, 1, 1)).astype(np.float32)\n",
    "    genres = ['blues','classical','country','disco','hiphop','jazz','metal','pop','reggae','rock']\n",
    "    labels = map(lambda x: np.float32(genres.index(x)), labels)\n",
    "    return dataset, labels\n",
    "\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Params\n",
    "learning_rate = 0.001\n",
    "training_iters = 100000\n",
    "batch_size = 4\n",
    "display_step = 20\n",
    "\n",
    "# Network Params\n",
    "n_input = 500 \n",
    "n_classes = 10 # genres \n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float32, [None, n_input])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "def conv2d(features, w, b):\n",
    "    return tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(features, w, strides=[1, 1, 1, 1], \n",
    "                                                  padding='SAME'),b))\n",
    "\n",
    "def max_pool(features, k):\n",
    "    return tf.nn.max_pool(features, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')\n",
    "\n",
    "def conv_net(_X, _weights, _biases):\n",
    "    _X = tf.reshape(_X, shape=[-1,, 1])\n",
    "\n",
    "    conv1 = conv2d(_X, _weights['wc1'], _biases['bc1'])\n",
    "    conv1 = max_pool(conv1, k=1)\n",
    "\n",
    "    conv2 = conv2d(conv1, _weights['wc2'], _biases['bc2'])\n",
    "    conv2 = max_pool(conv2, k=1)\n",
    "\n",
    "    dense1 = tf.reshape(conv2, [-1, _weights['wd1'].get_shape().as_list()[0]]) \n",
    "    dense1 = tf.nn.relu(tf.add(tf.matmul(dense1, _weights['wd1']), _biases['bd1']))\n",
    "    \n",
    "    out = tf.add(tf.matmul(dense1, _weights['out']), _biases['out'])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a huge field for improvements here:\n",
    "* Starting points should not be randomized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = {\n",
    "    # 5x5 conv, 1 input, 32 outputs\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 1, 1, 32])), \n",
    "    # 5x5 conv, 32 inputs, 64 outputs\n",
    "    'wc2': tf.Variable(tf.random_normal([5, 1, 32, 64])), \n",
    "    # fully connected, 7*7*64 inputs, 1024 outputs\n",
    "    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])), \n",
    "    # 1024 inputs, 10 outputs (class prediction)\n",
    "    'out': tf.Variable(tf.random_normal([1024, n_classes])) \n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'bc2': tf.Variable(tf.random_normal([64])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = conv_net(x, weights, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 513, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (500, 513, 1, 1) for Tensor u'Placeholder_6:0', which has shape '(?, 500)'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-6a74c5f375c4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mstep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mtraining_iters\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdisplay_step\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m             \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict)\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;33m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m`\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mdoesn\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0mt\u001b[0m \u001b[0mexist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m     \"\"\"\n\u001b[1;32m--> 315\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mpartial_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict)\u001b[0m\n\u001b[0;32m    504\u001b[0m                   \u001b[1;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    505\u001b[0m                   \u001b[1;34m'which has shape %r'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 506\u001b[1;33m                   % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n\u001b[0m\u001b[0;32m    507\u001b[0m           \u001b[0mfeed_dict_string\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp_val\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    508\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot feed value of shape (500, 513, 1, 1) for Tensor u'Placeholder_6:0', which has shape '(?, 500)'"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "    while step * batch_size < training_iters:        \n",
    "        sess.run(optimizer, feed_dict={x: train_dataset, y: train_labels})\n",
    "        if step % display_step == 0:\n",
    "            acc = sess.run(accuracy, feed_dict={x: train_dataset, y: train_labels})\n",
    "            loss = sess.run(cost, feed_dict={x: train_dataset, y: train_labels})\n",
    "            print(\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \"{:.6f}\".format(loss) + \\\n",
    "            \", Training Accuracy= \" + \"{:.5f}\".format(acc))\n",
    "        step += 1\n",
    "    print(\"Testing Accuracy:\", sess.run(accuracy, feed_dict={x: test_dataset, y: test_labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "2_fullyconnected.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
